{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neurawn/AnhITCHFeedHandler/blob/main/TinyImagenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uL0Admw7lVuG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-L9pFO-2bqw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8121357b-0f64-46db-ed8d-6704daa185b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount( '/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S8KqOL7J7Th",
        "outputId": "c4d8ded2-0e0c-4ad7-ddff-d61891655a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n03584254': 0, 'n02403003': 1, 'n02056570': 2, 'n02769748': 3, 'n01443537': 4, 'n02129165': 5, 'n02814533': 6, 'n04259630': 7, 'n01774750': 8, 'n02410509': 9}\n",
            "X_train shape:  (5000, 32, 32, 3)\n",
            "y_train shape:  (5000,)\n",
            "x_train shape: (5000, 32, 32, 3)\n",
            "5000 train samples\n",
            "500 test samples\n",
            "y_train shape: (5000,)\n",
            "2023-01-08 00:07:28.945736: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "312 resnet, input shape:  (None, 32, 32, 16) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 32, 32, 64) filter:  64\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "312 resnet, input shape:  (None, 32, 32, 64) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 16, 16, 128) filter:  128\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "312 resnet, input shape:  (None, 16, 16, 128) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "305 resnet, input shape after conv layer 2:  (None, 8, 8, 256) filter:  256\n",
            "Learning rate:  0.001\n",
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 16)   0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 16)   272         ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 16)   0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 64)   1088        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 32, 32, 64)   0           ['conv2d_4[0][0]',               \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 64)  256         ['add[0][0]']                    \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 16)   1040        ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 16)   0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 64)   1088        ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 32, 32, 64)   0           ['add[0][0]',                    \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 16)   1040        ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 16)   0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 32, 32, 64)   0           ['add_1[0][0]',                  \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 32, 16)   0           ['conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 64)   0           ['add_2[0][0]',                  \n",
            "                                                                  'conv2d_13[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 64)  256         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 32, 32, 16)   0           ['conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 32, 32, 16)   0           ['conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 32, 32, 64)   0           ['add_3[0][0]',                  \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 64)  256         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 32, 32, 16)   0           ['conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 32, 32, 16)   0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 32, 32, 64)   0           ['add_4[0][0]',                  \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 64)  256         ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 32, 32, 16)   0           ['conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 32, 32, 16)   0           ['conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 32, 32, 64)   0           ['add_5[0][0]',                  \n",
            "                                                                  'conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 64)  256         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 32, 32, 16)   0           ['conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 32, 32, 16)   0           ['conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 32, 32, 64)   0           ['add_6[0][0]',                  \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 64)  256         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 32, 32, 16)   1040        ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 32, 32, 16)   0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 32, 32, 16)   0           ['conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 32, 32, 64)   1088        ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 32, 32, 64)   0           ['add_7[0][0]',                  \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 64)  256         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 16, 16, 64)   4160        ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 16, 16, 64)   0           ['conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 16, 16, 64)   0           ['conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 16, 16, 128)  8320        ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 16, 16, 128)  0           ['conv2d_32[0][0]',              \n",
            "                                                                  'conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 128)  512        ['add_9[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 16, 16, 64)   0           ['conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 16, 16, 64)   0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 16, 16, 128)  0           ['add_9[0][0]',                  \n",
            "                                                                  'conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 16, 128)  512        ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 16, 16, 64)   0           ['conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 16, 16, 64)   0           ['conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 16, 16, 128)  0           ['add_10[0][0]',                 \n",
            "                                                                  'conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 128)  512        ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 16, 16, 64)   0           ['conv2d_39[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 16, 16, 64)   0           ['conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 16, 16, 128)  0           ['add_11[0][0]',                 \n",
            "                                                                  'conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 128)  512        ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 16, 16, 64)   0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 16, 16, 64)   0           ['conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 16, 16, 128)  0           ['add_12[0][0]',                 \n",
            "                                                                  'conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 128)  512        ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 16, 16, 64)   0           ['conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 16, 16, 64)   0           ['conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 16, 16, 128)  0           ['add_13[0][0]',                 \n",
            "                                                                  'conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 128)  512        ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 16, 16, 64)   0           ['conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 16, 16, 64)   0           ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 16, 16, 128)  0           ['add_14[0][0]',                 \n",
            "                                                                  'conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 128)  512        ['add_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 16, 16, 64)   0           ['conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 16, 16, 64)   0           ['conv2d_52[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " add_16 (Add)                   (None, 16, 16, 128)  0           ['add_15[0][0]',                 \n",
            "                                                                  'conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 128)  512        ['add_16[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 16, 16, 64)   8256        ['activation_51[0][0]']          \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 16, 16, 64)   0           ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 16, 16, 64)   36928       ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            " activation_53 (Activation)     (None, 16, 16, 64)   0           ['conv2d_55[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 16, 16, 128)  8320        ['activation_53[0][0]']          \n",
            "                                                                                                  \n",
            " add_17 (Add)                   (None, 16, 16, 128)  0           ['add_16[0][0]',                 \n",
            "                                                                  'conv2d_56[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 128)  512        ['add_17[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_54 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 8, 8, 128)    16512       ['activation_54[0][0]']          \n",
            "                                                                                                  \n",
            " activation_55 (Activation)     (None, 8, 8, 128)    0           ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_55[0][0]']          \n",
            "                                                                                                  \n",
            " activation_56 (Activation)     (None, 8, 8, 128)    0           ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 8, 8, 256)    33024       ['add_17[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_56[0][0]']          \n",
            "                                                                                                  \n",
            " add_18 (Add)                   (None, 8, 8, 256)    0           ['conv2d_60[0][0]',              \n",
            "                                                                  'conv2d_59[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 8, 8, 256)   1024        ['add_18[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_57 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_57[0][0]']          \n",
            "                                                                                                  \n",
            " activation_58 (Activation)     (None, 8, 8, 128)    0           ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_58[0][0]']          \n",
            "                                                                                                  \n",
            " activation_59 (Activation)     (None, 8, 8, 128)    0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_59[0][0]']          \n",
            "                                                                                                  \n",
            " add_19 (Add)                   (None, 8, 8, 256)    0           ['add_18[0][0]',                 \n",
            "                                                                  'conv2d_63[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 8, 8, 256)   1024        ['add_19[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_60 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_60[0][0]']          \n",
            "                                                                                                  \n",
            " activation_61 (Activation)     (None, 8, 8, 128)    0           ['conv2d_64[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_61[0][0]']          \n",
            "                                                                                                  \n",
            " activation_62 (Activation)     (None, 8, 8, 128)    0           ['conv2d_65[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_62[0][0]']          \n",
            "                                                                                                  \n",
            " add_20 (Add)                   (None, 8, 8, 256)    0           ['add_19[0][0]',                 \n",
            "                                                                  'conv2d_66[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 8, 8, 256)   1024        ['add_20[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_63 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_63[0][0]']          \n",
            "                                                                                                  \n",
            " activation_64 (Activation)     (None, 8, 8, 128)    0           ['conv2d_67[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_64[0][0]']          \n",
            "                                                                                                  \n",
            " activation_65 (Activation)     (None, 8, 8, 128)    0           ['conv2d_68[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_65[0][0]']          \n",
            "                                                                                                  \n",
            " add_21 (Add)                   (None, 8, 8, 256)    0           ['add_20[0][0]',                 \n",
            "                                                                  'conv2d_69[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 8, 8, 256)   1024        ['add_21[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_66 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_66[0][0]']          \n",
            "                                                                                                  \n",
            " activation_67 (Activation)     (None, 8, 8, 128)    0           ['conv2d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_67[0][0]']          \n",
            "                                                                                                  \n",
            " activation_68 (Activation)     (None, 8, 8, 128)    0           ['conv2d_71[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_68[0][0]']          \n",
            "                                                                                                  \n",
            " add_22 (Add)                   (None, 8, 8, 256)    0           ['add_21[0][0]',                 \n",
            "                                                                  'conv2d_72[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 8, 8, 256)   1024        ['add_22[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_69 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_69[0][0]']          \n",
            "                                                                                                  \n",
            " activation_70 (Activation)     (None, 8, 8, 128)    0           ['conv2d_73[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_70[0][0]']          \n",
            "                                                                                                  \n",
            " activation_71 (Activation)     (None, 8, 8, 128)    0           ['conv2d_74[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_71[0][0]']          \n",
            "                                                                                                  \n",
            " add_23 (Add)                   (None, 8, 8, 256)    0           ['add_22[0][0]',                 \n",
            "                                                                  'conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 8, 8, 256)   1024        ['add_23[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_72 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_72[0][0]']          \n",
            "                                                                                                  \n",
            " activation_73 (Activation)     (None, 8, 8, 128)    0           ['conv2d_76[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_73[0][0]']          \n",
            "                                                                                                  \n",
            " activation_74 (Activation)     (None, 8, 8, 128)    0           ['conv2d_77[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_74[0][0]']          \n",
            "                                                                                                  \n",
            " add_24 (Add)                   (None, 8, 8, 256)    0           ['add_23[0][0]',                 \n",
            "                                                                  'conv2d_78[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 8, 8, 256)   1024        ['add_24[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_75 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_75[0][0]']          \n",
            "                                                                                                  \n",
            " activation_76 (Activation)     (None, 8, 8, 128)    0           ['conv2d_79[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_76[0][0]']          \n",
            "                                                                                                  \n",
            " activation_77 (Activation)     (None, 8, 8, 128)    0           ['conv2d_80[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_77[0][0]']          \n",
            "                                                                                                  \n",
            " add_25 (Add)                   (None, 8, 8, 256)    0           ['add_24[0][0]',                 \n",
            "                                                                  'conv2d_81[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 8, 8, 256)   1024        ['add_25[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_78 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 8, 8, 128)    32896       ['activation_78[0][0]']          \n",
            "                                                                                                  \n",
            " activation_79 (Activation)     (None, 8, 8, 128)    0           ['conv2d_82[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 8, 8, 128)    147584      ['activation_79[0][0]']          \n",
            "                                                                                                  \n",
            " activation_80 (Activation)     (None, 8, 8, 128)    0           ['conv2d_83[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 8, 8, 256)    33024       ['activation_80[0][0]']          \n",
            "                                                                                                  \n",
            " add_26 (Add)                   (None, 8, 8, 256)    0           ['add_25[0][0]',                 \n",
            "                                                                  'conv2d_84[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 256)   1024        ['add_26[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_81 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 1, 1, 256)   0           ['activation_81[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 256)          0           ['average_pooling2d[0][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           2570        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,483,434\n",
            "Trainable params: 2,475,370\n",
            "Non-trainable params: 8,064\n",
            "__________________________________________________________________________________________________\n",
            "Using real-time data augmentation.\n",
            "Learning rate:  0.001\n",
            "Epoch 1/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 3.3302 - acc: 0.3438\n",
            "Epoch 1: val_acc improved from -inf to 0.10000, saving model to /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/saved_models_X/cifar10_ResNet83v2_model.001.h5\n",
            "157/157 [==============================] - 25s 67ms/step - loss: 3.3302 - acc: 0.3438 - val_loss: 3.9835 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 2/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 2.6685 - acc: 0.4748\n",
            "Epoch 2: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 2.6680 - acc: 0.4750 - val_loss: 4.1819 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 3/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 2.3768 - acc: 0.5133\n",
            "Epoch 3: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 2.3765 - acc: 0.5134 - val_loss: 4.4585 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 4/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 2.1798 - acc: 0.5455\n",
            "Epoch 4: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 2.1796 - acc: 0.5454 - val_loss: 4.0931 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 5/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 2.0173 - acc: 0.5769\n",
            "Epoch 5: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 2.0173 - acc: 0.5764 - val_loss: 3.9673 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 6/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.8991 - acc: 0.5896\n",
            "Epoch 6: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.8979 - acc: 0.5906 - val_loss: 4.1873 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 7/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.7959 - acc: 0.6051\n",
            "Epoch 7: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.7950 - acc: 0.6054 - val_loss: 5.8262 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 8/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.7235 - acc: 0.6170\n",
            "Epoch 8: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.7235 - acc: 0.6170 - val_loss: 3.0763 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 9/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.6590 - acc: 0.6238\n",
            "Epoch 9: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.6590 - acc: 0.6238 - val_loss: 6.5644 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 10/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.5949 - acc: 0.6386\n",
            "Epoch 10: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.5949 - acc: 0.6386 - val_loss: 3.9915 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 11/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.6600\n",
            "Epoch 11: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.5333 - acc: 0.6600 - val_loss: 3.0191 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 12/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.4808 - acc: 0.6645\n",
            "Epoch 12: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.4809 - acc: 0.6646 - val_loss: 4.6156 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 13/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.4370 - acc: 0.6693\n",
            "Epoch 13: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.4372 - acc: 0.6690 - val_loss: 4.8868 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 14/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.4290 - acc: 0.6718\n",
            "Epoch 14: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.4290 - acc: 0.6718 - val_loss: 5.7076 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 15/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.3813 - acc: 0.6789\n",
            "Epoch 15: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.3810 - acc: 0.6790 - val_loss: 4.9485 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 16/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.3399 - acc: 0.6868\n",
            "Epoch 16: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.3399 - acc: 0.6868 - val_loss: 5.4511 - val_acc: 0.1000 - lr: 3.1623e-04\n",
            "Learning rate:  0.001\n",
            "Epoch 17/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.3120 - acc: 0.6916\n",
            "Epoch 17: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.3120 - acc: 0.6916 - val_loss: 7.7044 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 18/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2935 - acc: 0.7022\n",
            "Epoch 18: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.2935 - acc: 0.7022 - val_loss: 6.3440 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 19/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2560 - acc: 0.7076\n",
            "Epoch 19: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.2560 - acc: 0.7076 - val_loss: 4.9526 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 20/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2186 - acc: 0.7180\n",
            "Epoch 20: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.2186 - acc: 0.7180 - val_loss: 4.3848 - val_acc: 0.1000 - lr: 0.0010\n",
            "Learning rate:  0.001\n",
            "Epoch 21/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.2166 - acc: 0.7220\n",
            "Epoch 21: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.2146 - acc: 0.7226 - val_loss: 4.0775 - val_acc: 0.1000 - lr: 3.1623e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 22/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.0846 - acc: 0.7641\n",
            "Epoch 22: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.0836 - acc: 0.7640 - val_loss: 4.6053 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 23/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.0042 - acc: 0.7895\n",
            "Epoch 23: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.0050 - acc: 0.7890 - val_loss: 5.1558 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 24/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 1.0000 - acc: 0.7888\n",
            "Epoch 24: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 1.0002 - acc: 0.7882 - val_loss: 5.3871 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 25/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9826 - acc: 0.7902\n",
            "Epoch 25: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.9826 - acc: 0.7902 - val_loss: 6.3337 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 26/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.9691 - acc: 0.8021\n",
            "Epoch 26: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.9687 - acc: 0.8024 - val_loss: 5.7955 - val_acc: 0.1000 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 27/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9471 - acc: 0.8084\n",
            "Epoch 27: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.9471 - acc: 0.8084 - val_loss: 6.0408 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 28/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9329 - acc: 0.8078\n",
            "Epoch 28: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.9329 - acc: 0.8078 - val_loss: 7.5209 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 29/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.9336 - acc: 0.8080\n",
            "Epoch 29: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.9343 - acc: 0.8080 - val_loss: 5.8515 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 30/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.8106\n",
            "Epoch 30: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.9200 - acc: 0.8106 - val_loss: 6.7042 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 31/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9277 - acc: 0.8092\n",
            "Epoch 31: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.9277 - acc: 0.8092 - val_loss: 6.7584 - val_acc: 0.1000 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 32/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.9010 - acc: 0.8174\n",
            "Epoch 32: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.9007 - acc: 0.8172 - val_loss: 6.6513 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 33/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9072 - acc: 0.8132\n",
            "Epoch 33: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.9072 - acc: 0.8132 - val_loss: 6.7217 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 34/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.9042 - acc: 0.8108\n",
            "Epoch 34: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.9042 - acc: 0.8108 - val_loss: 7.5800 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 35/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.8837 - acc: 0.8158\n",
            "Epoch 35: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.8833 - acc: 0.8154 - val_loss: 7.1128 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 36/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8645 - acc: 0.8246\n",
            "Epoch 36: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.8645 - acc: 0.8246 - val_loss: 6.3632 - val_acc: 0.1000 - lr: 3.1623e-05\n",
            "Learning rate:  0.0001\n",
            "Epoch 37/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8622 - acc: 0.8266\n",
            "Epoch 37: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.8622 - acc: 0.8266 - val_loss: 5.2085 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 38/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8729 - acc: 0.8274\n",
            "Epoch 38: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.8729 - acc: 0.8274 - val_loss: 7.3520 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 39/40\n",
            "156/157 [============================>.] - ETA: 0s - loss: 0.8590 - acc: 0.8301\n",
            "Epoch 39: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.8593 - acc: 0.8300 - val_loss: 5.8372 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Learning rate:  0.0001\n",
            "Epoch 40/40\n",
            "157/157 [==============================] - ETA: 0s - loss: 0.8476 - acc: 0.8276\n",
            "Epoch 40: val_acc did not improve from 0.10000\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.8476 - acc: 0.8276 - val_loss: 5.7406 - val_acc: 0.1000 - lr: 1.0000e-04\n",
            "Test loss: 5.740607261657715\n",
            "Test accuracy: 0.10000000149011612\n"
          ]
        }
      ],
      "source": [
        "!cd /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200 && python \"resnet.py\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCdgOab4DL4D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "THIS LOAD DATA IS STRICTLY FOR MODULE_V1\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# from scipy.misc import imread\n",
        "\n",
        "num_classes = 10\n",
        "root_dir_path = \"/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/\"\n",
        "wnids_path = root_dir_path + \"wnids\"\n",
        "words_path = root_dir_path + \"words\"\n",
        "\n",
        "def load_tiny_imagenet(path, resize='False', dtype=np.float32):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - path: String giving path to the directory to load.\n",
        "    - dtype: numpy datatype used to load the data.\n",
        "\n",
        "    Returns: A tuple of\n",
        "    - class_names: A list where class_names[i] is a list of strings giving the\n",
        "        WordNet names for class i in the loaded dataset.\n",
        "    - X_train: (N_tr, 3, 64, 64) array of training images\n",
        "    - y_train: (N_tr,) array of training labels\n",
        "    - X_val: (N_val, 3, 64, 64) array of validation images\n",
        "    - y_val: (N_val,) array of validation labels\n",
        "    - X_test: (N_test, 3, 64, 64) array of testing images.\n",
        "    - y_test: (N_test,) array of test labels; if test labels are not available\n",
        "        (such as in student code) then y_test will be None.\n",
        "    \"\"\"\n",
        "    # First load wnids\n",
        "    wnids_file = wnids_path + str(num_classes) + '.txt'\n",
        "    with open(wnids_file, 'r') as f:\n",
        "        wnids = [x.strip() for x in f]\n",
        "\n",
        "    # Map wnids to integer labels\n",
        "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
        "    print(wnid_to_label)\n",
        "\n",
        "# \t# Use words.txt to get names for each class\n",
        "    # words_file = words_path + str(num_classes) + '.txt'\n",
        "    words_file = words_path + '.txt'\n",
        "    with open(words_file, 'r') as f:\n",
        "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
        "        for wnid, words in wnid_to_words.items():\n",
        "            wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
        "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
        "\n",
        "    # Next load training data.\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i, wnid in enumerate(wnids):\n",
        "        # To figure out the filenames we need to open the boxes file\n",
        "        boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
        "        with open(boxes_file, 'r') as f:\n",
        "            filenames = [x.split('\\t')[0] for x in f]\n",
        "        num_images = len(filenames)\n",
        "\n",
        "        # X_train_block = np.zeros((num_images, 64, 64, 3), dtype=dtype)\t\n",
        "        X_train_block = np.zeros((num_images, 32, 32, 3), dtype=dtype)\n",
        "        y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n",
        "\n",
        "        for j, image_path in enumerate(filenames):\n",
        "            image_path = os.path.join(path, 'train', wnid, 'images', image_path)\n",
        "            image = Image.open(image_path)\n",
        "            # image = image.resize((32, 32), Image.LANCZOS)\n",
        "            image_array = []\n",
        "            if (image.mode == \"RGB\"):\n",
        "                image_array = np.array(image, dtype = 'uint8')\n",
        "            else: # Grayscale:\n",
        "                image_array = np.stack((image,) * 3, axis = -1)\n",
        "            \n",
        "            X_train_block[j] = image_array\n",
        "        X_train.append(X_train_block)\n",
        "        y_train.append(y_train_block)\n",
        "\n",
        "        print(\"finish loading \", image_path)\n",
        "    \n",
        "            \n",
        "    # We need to concatenate all training data\n",
        "    X_train = np.concatenate(X_train, axis=0)\n",
        "    y_train = np.concatenate(y_train, axis=0)\n",
        "    # y_train = y_train.reshape([5000, 1])\n",
        "\n",
        "    # Next load validation data\n",
        "    with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
        "        image_files = []\n",
        "        val_wnids = []\n",
        "        for line in f:\n",
        "            # Select only validation images in chosen wnids set\n",
        "            if line.split()[1] in wnids:\n",
        "                image_file, wnid = line.split('\\t')[:2]\n",
        "                image_files.append(image_file)\n",
        "                val_wnids.append(wnid)\n",
        "\n",
        "        num_val = len(image_files)\n",
        "        \n",
        "        # X_val = np.zeros((num_val, 64, 64, 3), dtype=dtype)\n",
        "        X_val = np.zeros((num_val, 32, 32, 3), dtype=dtype)\n",
        "        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
        " \n",
        "        for i, image_file in enumerate(image_files):\n",
        "            image_file = os.path.join(path, 'val', 'images', image_file)\n",
        "            image = Image.open(image_path)\n",
        "            # image = image.resize((32, 32), Image.LANCZOS)\n",
        "            image_array = []\n",
        "            if (image.mode == \"RGB\"):\n",
        "                image_array = np.array(image, dtype = 'uint8')\n",
        "            else: # Grayscale:\n",
        "                image_array = np.stack((image,) * 3, axis = -1)\n",
        "\n",
        "            X_val[i] = image_array\n",
        "\n",
        "    return class_names, X_train, y_train, X_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CVexPyMtSWE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import add, AveragePooling2D\n",
        "from tensorflow.keras.models import load_model\n",
        "class util:\n",
        "    def __init__(self):\n",
        "        self.convW =[]\n",
        "        self.convB = []\n",
        "        self.batchGamma = []\n",
        "        self.batchBeta = []\n",
        "        self.dense1W = []\n",
        "        self.dense1B = []\n",
        "        self.b1 = None\n",
        "        self.b2 = None\n",
        "        self.b3 = None\n",
        "        self.D1 = None\n",
        "        self.D2 = None\n",
        "        self.D3 = None\n",
        "        self.d1 = None\n",
        "        self.d2 = None\n",
        "        self.d3 = None\n",
        "        self.first = True\n",
        "        self.convWList = []\n",
        "        self.convBList = []\n",
        "        self.convMap = []\n",
        "        self.X1proposed = None\n",
        "        self.X1Previous = None\n",
        "        self.first_use = True\n",
        "               \n",
        "    def getweights(self, nm):\n",
        "        for i in range(0,len(nm.layers)):\n",
        "            if(\"conv\" in nm.layers[i].name):\n",
        "                if (np.vstack([nm.layers[i].get_weights()[0]]).shape[0] == 3):\n",
        "                    self.convW.append(np.vstack([nm.layers[i].get_weights()[0]]))\n",
        "                    self.convB.append(nm.layers[i].get_weights()[1])\n",
        "                # print (\"shape: \", np.vstack([nm.layers[i].get_weights()[0]]).shape)\n",
        "        for i in range(0,len(nm.layers)):\n",
        "            if(\"batch\" in nm.layers[i].name):\n",
        "                self.batchGamma.append(nm.layers[i].get_weights()[0])\n",
        "                self.batchBeta.append(nm.layers[i].get_weights()[1])\n",
        "        for i in range(0,len(nm.layers)):\n",
        "            if(\"dense\" in nm.layers[i].name):\n",
        "                self.dense1W=np.vstack([nm.layers[i].get_weights()[0]])\n",
        "                self.dense1B=nm.layers[i].get_weights()[1]\n",
        "        self.batchGamma.append(0)\n",
        "        self.batchBeta.append(1)\n",
        "        self.D2 = np.zeros_like(self.dense1W)\n",
        "        self.d2 = np.zeros_like(self.dense1B)\n",
        "        return self.convW, self.convB, self.batchGamma, self.batchBeta, self.dense1W, self.dense1B\n",
        "\n",
        "    def softmax(self,x):\n",
        "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "    def find(self,lst):\n",
        "        return [i for i, x in enumerate(lst) if x<=0]\n",
        "    def find2(self,lst):\n",
        "        return [i for i, x in enumerate(lst) if x>0]\n",
        "\n",
        "    def convStrictOperation(self, x, W, B, number, strides):\n",
        "        convX= self.conv2d(x, W,  stride = (strides,strides)) + B\n",
        "        if(self.first_use==False):\n",
        "            mapTemp=self.convMap[number]\n",
        "        else:\n",
        "            mapTemp=[]\n",
        "        #WTemp=[]\n",
        "        #BTemp=[]\n",
        "        convXtemp=convX.reshape(convX.shape[0]*convX.shape[1]*convX.shape[2]*convX.shape[3])\n",
        "        zeroConvX=self.find(convXtemp)\n",
        "        if(self.first_use==True):\n",
        "            mapTemp=zeroConvX\n",
        "        else:\n",
        "            positiveConvX=self.find2(convXtemp)\n",
        "            notPresentlist=list(set(positiveConvX).intersection(self.convMap[number]))\n",
        "            for i in notPresentlist:\n",
        "                indexItem=self.convMap[number].index(i)\n",
        "                del self.convMap[number][indexItem] # removing inactive nodes in the layer.\n",
        "        return convX, mapTemp, #WTemp, BTemp\n",
        "\n",
        "    def denseOperation(self, x, W, B, D,d):\n",
        "        x=np.dot(x,W)\n",
        "        x=np.add(x, B)\n",
        "        x[x<0]=0\n",
        "        for i in range(x.shape[0]):\n",
        "            if x[i] <= 0:\n",
        "                D[:,i] = [x for x in D[:,i]]\n",
        "            else:\n",
        "                D[:,i] = W[:,i]\n",
        "                d[i] = B[i]\n",
        "        return x, D, d\n",
        "    \n",
        "    def copyWeightandBias(self,W1,b1,x,strides):\n",
        "        #X = img.reshape((1,32,32,3)) \n",
        "        W1=np.vstack([W1])\n",
        "        X1=self.conv2d(x, W1,  stride = (strides,strides)) + b1\n",
        "        X21=X1.reshape(X1.shape[0]*X1.shape[1]*X1.shape[2]*X1.shape[3])\n",
        "        convB=[]\n",
        "        convW=[]\n",
        "        for i in range(0,len(X21)):\n",
        "            convW.append(W1)\n",
        "            convB.append(b1)\n",
        "        return convW, convB\n",
        "\n",
        "    def resnet_layer_Predict(self,inputs, W, B, Gamma, Beta,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "        \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    \n",
        "        Arguments:\n",
        "            inputs (tensor): input tensor from input image or previous layer\n",
        "            strides (int): Conv2D square stride dimensions\n",
        "            activation (string): activation name\n",
        "            batch_normalization (bool): whether to include batch normalization\n",
        "            conv_first (bool): conv-bn-activation (True) or\n",
        "                bn-activation-conv (False)\n",
        "    \n",
        "        Returns:\n",
        "            x (tensor): tensor as input to the next layer\n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        if conv_first:\n",
        "            #x = self.convPredict(x, W, B, strides)\n",
        "            x = self.conv2d(inputs, W, stride = (strides,strides)) + B\n",
        "\n",
        "            if batch_normalization:\n",
        "                x = self.batchnorm_forward(x,Gamma, Beta)\n",
        "            if activation is not None:\n",
        "                x = self.relu(x)\n",
        "        else:\n",
        "            if batch_normalization:\n",
        "                x = self.batchnorm_forward(x,Gamma, Beta)\n",
        "            if activation is not None:\n",
        "                x = self.relu(x)\n",
        "            #x = self.convPredict(x, W, B, strides)\n",
        "            x = self.conv2d(x, W, stride = (strides, strides)) + B\n",
        "        return x\n",
        "\n",
        "    def simplePredict(self,inputs, input_shape, convW, convB, GammaList, BetaList, denseW, denseB, num_classes=10,depth=0):\n",
        "        \"\"\"ResNet Version 1 Model builder [a]\n",
        "    \n",
        "        Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "        Last ReLU is after the shortcut connection.\n",
        "        At the beginning of each stage, the feature map size is halved\n",
        "        (downsampled) by a convolutional layer with strides=2, while \n",
        "        the number of filters is doubled. Within each stage, \n",
        "        the layers have the same number filters and the\n",
        "        same number of filters.\n",
        "        Features maps sizes:\n",
        "        stage 0: 32x32, 16\n",
        "        stage 1: 16x16, 32\n",
        "        stage 2:  8x8,  64\n",
        "        The Number of parameters is approx the same as Table 6 of [a]:\n",
        "        ResNet20 0.27M\n",
        "        ResNet32 0.46M\n",
        "        ResNet44 0.66M\n",
        "        ResNet56 0.85M\n",
        "        ResNet110 1.7M\n",
        "    \n",
        "        Arguments:\n",
        "            input_shape (tensor): shape of input image tensor\n",
        "            depth (int): number of core convolutional layers\n",
        "            num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    \n",
        "        Returns:\n",
        "            model (Model): Keras model instance\n",
        "        \"\"\"\n",
        "        if (depth - 2) % 6 != 0:\n",
        "            raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
        "        # start model definition.\n",
        "        num_filters = 16\n",
        "        num_res_blocks = int((depth - 2) / 6)\n",
        "        i=0\n",
        "        j=0\n",
        "        \n",
        "        inputs  = inputs.reshape((1, inputs.shape[0], inputs.shape[1], inputs.shape[2]))\n",
        "        x       = self.resnet_layer_Predict(inputs,convW[i], convB[i], GammaList[j], GammaList[j], batch_normalization=False)\n",
        "\n",
        "        if self.first_use:\n",
        "            convX, mapList = self.convStrictOperation(inputs, np.vstack([convW[i]]), convB[i], i, 1)\n",
        "            self.convMap.append(mapList)\n",
        "        else:\n",
        "            convX, self.convMap[i] = self.convStrictOperation(inputs, np.vstack([convW[i]]), convB[i], i, 1)\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "        # instantiate the stack of residual units\n",
        "        for stack in range(3):\n",
        "            for res_block in range(num_res_blocks):\n",
        "                strides = 1\n",
        "                # first layer but not first stack\n",
        "                if stack > 0 and res_block == 0:  \n",
        "                    strides = 2  # downsample\n",
        "                if self.first_use:\n",
        "                    convY, mapList = self.convStrictOperation(x, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                    self.convMap.append(mapList)\n",
        "                else:\n",
        "                    convY, self.convMap[i]=self.convStrictOperation(x, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                y = self.resnet_layer_Predict(x, convW[i], convB[i], GammaList[j], GammaList[j], strides=strides, batch_normalization=False)\n",
        "\n",
        "                i=i+1\n",
        "                \n",
        "                if self.first_use:\n",
        "                    convY, mapList = self.convStrictOperation(y, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                    self.convMap.append(mapList)\n",
        "                else:\n",
        "                    convY, self.convMap[i] = self.convStrictOperation(y, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                y = self.resnet_layer_Predict(y, convW[i], convB[i], GammaList[j], GammaList[j], activation=None, batch_normalization=False)\n",
        "                i=i+1\n",
        "                #j=j+1\n",
        "                # first layer but not first stack\n",
        "                if stack > 0 and res_block == 0:\n",
        "                    if self.first_use:\n",
        "                        convX, mapList = self.convStrictOperation(x, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                        self.convMap.append(mapList)\n",
        "                    else:\n",
        "                        convX, self.convMap[i]=self.convStrictOperation(x, np.vstack([convW[i]]), convB[i], i, strides)\n",
        "                    x = self.resnet_layer_Predict(x, convW[i], convB[i], GammaList[j], GammaList[j], \n",
        "                                     strides=strides,\n",
        "                                     activation=None,\n",
        "                                     batch_normalization=False)\n",
        "                    i=i+1\n",
        "                x = add([y, x])\n",
        "                x = self.relu(x)\n",
        "    \n",
        "        # add classifier on top.\n",
        "        # v1 does not use BN after last shortcut connection-ReLU\n",
        "        x=AveragePooling2D(pool_size=8)(x).numpy()\n",
        "        y = x.reshape(x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3])\n",
        "        denseX, self.D2, self.d2 = self.denseOperation(y, denseW, denseB, self.D2, self.d2)\n",
        "        x = np.dot(x,denseW)\n",
        "        x = np.add(x,denseB)\n",
        "        self.X1proposed=x\n",
        "        x = self.softmax(x.reshape(x.shape[3]))\n",
        "        self.first_use=False\n",
        "        # instantiate model.\n",
        "        #model = Model(inputs=inputs, outputs=outputs)\n",
        "        return self.convMap, self.D2, self.d2                           \n",
        "\n",
        "    def relu(self,X):\n",
        "        return np.maximum(X, 0)\n",
        "\n",
        "    def batchnorm_forward(self,X, gamma = 1, beta = 0):\n",
        "        mu = np.mean(X, axis=0)\n",
        "        var = np.var(X, axis=0)\n",
        "    \n",
        "        X_norm = (X - mu) / np.sqrt(var + 1e-8)\n",
        "        out = gamma * X_norm + beta\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def conv2d(self, x, w, pad='SAME', stride=(1, 1)):\n",
        "        \"\"\"2D convolution (technically speaking, correlation).\n",
        "    \n",
        "        Args:\n",
        "            x: [N, H, W, C]\n",
        "            w: [I, J, C, K]\n",
        "            pad: [PH, PW]\n",
        "            stride: [SH, SW]\n",
        "    \n",
        "        Returns:\n",
        "            y: [N, H', W', K]\n",
        "        \"\"\"\n",
        "        ksize = w.shape[:2]\n",
        "        x = self.extract_sliding_windows(x, ksize, pad, stride)\n",
        "        \n",
        "        ws = w.shape\n",
        "        w = w.reshape([ws[0] * ws[1] * ws[2], ws[3]])\n",
        "        xs = x.shape\n",
        "        x = x.reshape([xs[0] * xs[1] * xs[2], -1])\n",
        "        y = x.dot(w)\n",
        "        y = y.reshape([xs[0], xs[1], xs[2], -1])\n",
        "        return y\n",
        "\n",
        "    def extract_sliding_windows(self,x, ksize, pad, stride, floor_first=True):\n",
        "        \"\"\"Converts a tensor to sliding windows.\n",
        "    \n",
        "        Args:\n",
        "            x: [N, H, W, C]\n",
        "            k: [KH, KW]\n",
        "            pad: [PH, PW]\n",
        "            stride: [SH, SW]\n",
        "    \n",
        "        Returns:\n",
        "            y: [N, (H-KH+PH+1)/SH, (W-KW+PW+1)/SW, KH * KW, C]\n",
        "        \"\"\"\n",
        "        n = x.shape[0]\n",
        "        h = x.shape[1]\n",
        "        w = x.shape[2]\n",
        "        c = x.shape[3]\n",
        "        kh = ksize[0]\n",
        "        kw = ksize[1]\n",
        "        sh = stride[0]\n",
        "        sw = stride[1]\n",
        "    \n",
        "        h2 = int(self.calc_size(h, kh, pad, sh))\n",
        "        w2 = int(self.calc_size(w, kw, pad, sw))\n",
        "        ph = int(self.calc_pad(pad, h, h2, sh, kh))\n",
        "        pw = int(self.calc_pad(pad, w, w2, sw, kw))\n",
        "    \n",
        "        ph0 = int(np.floor(ph / 2))\n",
        "        ph1 = int(np.ceil(ph / 2))\n",
        "        pw0 = int(np.floor(pw / 2))\n",
        "        pw1 = int(np.ceil(pw / 2))\n",
        "        if(ph0==-1):\n",
        "            ph0=0\n",
        "        if(pw0==-1):\n",
        "            pw0=0\n",
        "        if floor_first:\n",
        "            pph = (ph0, ph1)\n",
        "            ppw = (pw0, pw1)\n",
        "        else:\n",
        "            pph = (ph1, ph0)\n",
        "            ppw = (pw1, pw0)\n",
        "        x = np.pad(\n",
        "            x, ((0, 0), pph, ppw, (0, 0)),\n",
        "            mode='constant',\n",
        "            constant_values=(0.0, ))\n",
        "    \n",
        "        # The following code extracts window without copying the data:\n",
        "        # y = np.zeros([n, h2, w2, kh, kw, c])\n",
        "        # for ii in range(h2):\n",
        "        #     for jj in range(w2):\n",
        "        #         xx = ii * sh\n",
        "        #         yy = jj * sw\n",
        "        #         y[:, ii, jj, :, :, :] = x[:, xx:xx + kh, yy:yy + kw, :]\n",
        "        x_sn, x_sh, x_sw, x_sc = x.strides\n",
        "        y_strides = (x_sn, sh * x_sh, sw * x_sw, x_sh, x_sw, x_sc)\n",
        "        y = np.ndarray((n, h2, w2, kh, kw, c),\n",
        "                       dtype=x.dtype,\n",
        "                       buffer=x.data,\n",
        "                       offset=self.array_offset(x),\n",
        "                       strides=y_strides)\n",
        "        return y\n",
        "    def softmax(self,x):\n",
        "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "    def calc_size(self, h, kh, pad, sh):\n",
        "        \"\"\"Calculate output image size on one dimension.\n",
        "    \n",
        "        Args:\n",
        "            h: input image size.\n",
        "            kh: kernel size.\n",
        "            pad: padding strategy.\n",
        "            sh: stride.\n",
        "    \n",
        "        Returns:\n",
        "            s: output size.\n",
        "        \"\"\"\n",
        "    \n",
        "        if pad == 'VALID':\n",
        "            return np.ceil((h - kh + 1) / sh)\n",
        "        elif pad == 'SAME':\n",
        "            return np.ceil(h / sh)\n",
        "        else:\n",
        "            return int(np.ceil((h - kh + pad + 1) / sh))\n",
        "    def calc_pad(self,pad, in_siz, out_siz, stride, ksize):\n",
        "        \"\"\"Calculate padding width.\n",
        "    \n",
        "        Args:\n",
        "            pad: padding method, \"SAME\", \"VALID\", or manually speicified.\n",
        "            ksize: kernel size [I, J].\n",
        "    \n",
        "        Returns:\n",
        "            pad_: Actual padding width.\n",
        "        \"\"\"\n",
        "        if pad == 'SAME':\n",
        "            return (out_siz - 1) * stride + ksize - in_siz\n",
        "        elif pad == 'VALID':\n",
        "            return 0\n",
        "        else:\n",
        "            return pad\n",
        "    \n",
        "    def array_offset(self,x):\n",
        "        \"\"\"Get offset of array data from base data in bytes.\"\"\"\n",
        "        if x.base is None:\n",
        "            return 0\n",
        "    \n",
        "        base_start = x.base.__array_interface__['data'][0]\n",
        "        start = x.__array_interface__['data'][0]\n",
        "        return start - base_start "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac-rOiKICXEz",
        "outputId": "c6462281-4fab-4e42-e99d-a6c803500bf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n03584254': 0, 'n02403003': 1, 'n02056570': 2, 'n02769748': 3, 'n01443537': 4, 'n02129165': 5, 'n02814533': 6, 'n04259630': 7, 'n01774750': 8, 'n02410509': 9}\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n03584254/images/n03584254_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02403003/images/n02403003_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02056570/images/n02056570_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02769748/images/n02769748_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n01443537/images/n01443537_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02129165/images/n02129165_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02814533/images/n02814533_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n04259630/images/n04259630_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n01774750/images/n01774750_499.JPEG\n",
            "finish loading  /content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/train/n02410509/images/n02410509_499.JPEG\n",
            "yT shape: (5000,) yt shape: (500,)\n",
            "input shape:  (32, 32, 3)\n",
            "Start Time:14:39:47\n",
            "#Module 0 in progress....\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-faea0950caf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdigit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmaplist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplePredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchGamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchBeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense1W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense1B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# print('+ve Here, mc: ', mc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-05ad0d793e18>\u001b[0m in \u001b[0;36msimplePredict\u001b[0;34m(self, inputs, input_shape, convW, convB, GammaList, BetaList, denseW, denseB, num_classes, depth)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_use\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                     \u001b[0mconvY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvStrictOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconvW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvMap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-05ad0d793e18>\u001b[0m in \u001b[0;36mconvStrictOperation\u001b[0;34m(self, x, W, B, number, strides)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvStrictOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mconvX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_use\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mmapTemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-05ad0d793e18>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(self, x, w, pad, stride)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1024,144) and (576,64) not aligned: 144 (dim 1) != 576 (dim 0)"
          ]
        }
      ],
      "source": [
        "# from utils.cifarUtil import cifarUtilClass\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import datetime\n",
        "# from load_data import load_tiny_imagenet\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "# In[]: Data Acquisition and Model Load\n",
        "path = \"/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/\"\n",
        "model=load_model('/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/resnet56_v1Full_test.h5')\n",
        "class_name, xT, yT, xt, yt = load_tiny_imagenet(path)\n",
        "print (\"yT shape:\", yT.shape, \"yt shape:\", yt.shape)\n",
        "# In[]: Module Creation\n",
        "test=[]\n",
        "accuracyCheck=[]\n",
        "nonDigit=[]\n",
        "resNet=57\n",
        "labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "input_shape = xt.shape[1:]\n",
        "print (\"input shape: \", input_shape)\n",
        "print(\"Start Time:\"+datetime.now().strftime(\"%H:%M:%S\"))\n",
        "for j in labels:\n",
        "    print(\"#Module \"+str(j)+\" in progress....\")\n",
        "    digit  = []\n",
        "    slc = util()\n",
        "    convW, convB, batchGamma, batchBeta, dense1W, dense1B=slc.getweights(model)\n",
        "    for i in range(0,len(yt)):\n",
        "        if yT[i] == j and model.predict(xT[i:i+1], verbose = 0)[0][j] >.9:\n",
        "            digit.append(xT[i])\n",
        "\n",
        "    digit = digit[0:10]\n",
        "    mc = 0\n",
        "    for x in digit:\n",
        "        maplist, D2, d2 = slc.simplePredict(x, input_shape, convW, convB, batchGamma, batchBeta, dense1W, dense1B, depth=56)\n",
        "        mc = mc+1\n",
        "        # print('+ve Here, mc: ', mc)\n",
        "        if np.count_nonzero(slc.D2) < 45:\n",
        "            #print(\"Breaking at mc \", mc,np.count_nonzero(slc.D2))\n",
        "            slc.first = True\n",
        "\n",
        "    digit=[]\n",
        "\n",
        "    for k in labels:\n",
        "        if(k!=j):\n",
        "            temp=xT[yT==k]\n",
        "            temp=temp[0:10]\n",
        "            for u in temp:\n",
        "                digit.append(u)\n",
        "\n",
        "    np.random.shuffle(digit)\n",
        "    mc = 0\n",
        "    for x in digit:\n",
        "        maplist, D2, d2 = slc.simplePredict(x, input_shape, convW, convB, batchGamma, batchBeta, dense1W, dense1B, depth=56)\n",
        "        mc = mc+1\n",
        "        # print('-ve Here, mc: ', mc)\n",
        "        if np.count_nonzero(slc.D3) < 45:\n",
        "            #print(\"Breaking at mc \", mc,np.count_nonzero(slc.D2))\n",
        "            slc.first = True\n",
        "    #In[]: Verifiy\n",
        "    for i in range(0, resNet):\n",
        "        with open(\"/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/modeltext/Module\"+str(j)+\"-\"+str(i)+\"Map.txt\", \"w+\") as fp:   #Pickling\n",
        "            print (\"j: \" + str(j) + \"maplist[\" + str(i)+ \"]\", maplist[i])\n",
        "            # fp.write(\"\\n\".join(str(item) for item in maplist[i]))\n",
        "    # with open(\"/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/modeltext/Module\"+str(j)+\"DenseW.txt\", \"w+\") as fp:   #Pickling\n",
        "        print(\"j: \" + str(j) + \"D2: \", D2)\n",
        "        # fp.write(D2)\n",
        "    # with open(\"/content/gdrive/MyDrive/ColabNotebooks/Decomposer_2/ImageNet/tiny-imagenet-200/modeltext/Module\"+str(j)+\"DenseB.txt\", \"w+\") as fp:   #Pickling\n",
        "        print(\"j: \" + str(j) + \"d2: \", d2)\n",
        "        # fp.write(d2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX73qCCeCz-R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}